---
tags:
  - DataScience
  - LLM
  - MachineLearning
  - ArtificialIntelligence
---
# GPT-3 dataset detailsâ€ 
![[pretraining-dataset-of-popular-gpt-3-llm.png]]

>[!tip] "Number of tokens" col total ~499 billion, but the model was trained only on 300 billion tokens (â‡’ 60% of the original data).

- 410 billion tokens from CommonCrawl dataset requires ~570GB of storage (<i>perhaps, not a lot </i>ðŸ¤”).
- Later iterations of GPT-3 like models such as Meta's Llama expanded training scope to 
	- Arxiv research papers (92GB).
	- StackExchange's code-related Q&As (78 GB).
- GPT-3 paper authors did not share the training dataset.

*Sources: 
- â€ [Build a Large Language Model from Scratch](https://www.amazon.sg/dp/1633437167?ref_=mr_direct_us_sg_sg&showmri)

# GPT architecture
- GPT was originally introduced in the paper â€œImproving Language Understanding by Generative Pre-Trainingâ€ (https://mng.bz/x2qg) by Radford et al. from OpenAI. 
- GPT-3 is a scaled-up version of this model that has more parameters and was trained on a larger dataset. 
- In addition, the original model offered in ChatGPT was created by fine-tuning GPT-3 on a large instruction dataset using a method from OpenAIâ€™s InstructGPT paper (https://arxiv.org/abs/2203.02155).

>[!tip] GPT-3 has 96 transformer layers and 175 billion parametersâ€ .

*Sources: 
- â€ [Build a Large Language Model from Scratch](https://www.amazon.sg/dp/1633437167?ref_=mr_direct_us_sg_sg&showmri)

# GPT-3 pre-training cost

> [!tip] Estimated to be around $4.5 million in terms of cloud computing creditsâ€¡.

*Sources:* 
- â€¡[[D] GPT-3, The $4,600,000 Language Model | Reddit](https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/)
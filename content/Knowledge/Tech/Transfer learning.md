---
tags:
  - DataScience
---

### Key Differences between Transfer Learning and [[Fine-tuning|Fine-Tuning]]

| **Aspect**            | **Transfer Learning**                                                                                                                                              | [[Fine-tuning\|**Fine-Tuning**]]                                                                                                                                           |
| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Block diagram         | ![Transfer Learning](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg)                             | ![Fine-tuning from pre-training](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg)                          |
| **Scope**             | Broad concept of reusing a pre-trained model for a new task                                                                                                        | Specific technique of adapting a pre-trained model                                                                                                                         |
| **Process**           | Often involves using the pre-trained model as-is or modifying the last layer                                                                                       | Involves additional training with a new dataset                                                                                                                            |
| **Layer Adjustments** | Generally keeps all layers, sometimes adds a new output layer                                                                                                      | Freezes early layers, trains later ones (or the entire model)                                                                                                              |
| **Goal**              | Leverage general features learned by the pre-trained model                                                                                                         | Make the model specific to the new taskâ€™s requirements                                                                                                                     |
| Purpose               | Reusing a pre-trained model to avoid training from scratch, leveraging learned patterns (such as visual features, language structures, etc.) from the source task. | Adapt the pre-trained model to perform better on the target task by allowing some degree of learning specific to the new dataset.                                          |
| **Example**           | Using a model trained on ImageNet to perform object detection or image classification on a different dataset.                                                      | Using BERT [[Pre-training\|pre-trained]] on general text data and [[Fine-tuning\|fine-tuning]] it on a specific task, like sentiment analysis or named entity recognition. |
